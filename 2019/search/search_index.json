{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Advanced Methods in Applied Statistics 2019","text":""},{"location":"#the-educators","title":"The Educators","text":"<ul> <li>D. Jason Koskinen Lecturer     Email: <code>koskinen (at) nbi.ku.dk</code> </li> <li>Jean-Loup Teaching Assistant </li> </ul>"},{"location":"#basic-information","title":"Basic Information","text":"<ul> <li>Block 3 - Timetable A of the 2019 academic calendar<ul> <li>Tues 08:00 - 12:00 and Thurs 08:00- 12:00 &amp; 13:00 - 17:00</li> <li>Actual<ul> <li>08:30 - 09:00 Q&amp;A or discussion with Jason in the classroom</li> <li>09:00 lecture on new material (not 09:05 or 09:15)</li> <li>On Thursday there will very often be new material starting at 13:00</li> <li>On Thursday it is very unlikely that any new material, lectures, or review will happen after 16:00.</li> </ul> </li> </ul> </li> <li>There are multiple locations depending on the day (timetable):<ul> <li>Tuesday is at oev - bib 4-0-17, Universitetsparken 1-3, DIKU</li> <li>Thursday morning is at Aud 10, Universitetsparken 5, HCOE</li> <li>Thursday afternoon is at Aud 06, Universitetsparken 5, HCOE</li> </ul> </li> <li>Odd-numbered classes are 4-hours while even-numbered consist of 2 blocks of 4-hours</li> <li>Classes will be composed of ~20-30% lecture and demonstrations followed by exercise</li> <li>While assignments, projects, and exercises can be done in the programming language of the students choice, the examples and demonstrations will be mainly in Python and/or scientific packages thereof, i.e. SciPy, PyROOT, etc.</li> <li>Required text or textbooks: None</li> <li>Previous course pages:<ul> <li>2016 Advanced Methods in Applied Statistics webpage</li> <li>2017 Advanced Methods in Applied Statistics webpage</li> <li>2018 Advanced Methods in Applied Statistics webpage</li> </ul> </li> <li>It is recommended, but not required, to have at least reviewed the little sibling to this course, i.e. \"Applied Statistics - From data to results\" which can be found here</li> </ul>"},{"location":"#evaluation","title":"Evaluation","text":""},{"location":"#oral-presentation","title":"Oral presentation and 1-2 page summary (10%)","text":"<ul> <li>~10 minute summary presentation. Plan on ~7 slides if you are doing a PowerPoint-type presentation.</li> <li>Can work alone or in groups of up to 3.<ul> <li>A 1-2 page summary including any and all group members names</li> <li>I strongly encourage people to use LaTeX for the typesetting of the written summary. For those who do not already have a style, I would recommend trying the formatting style for submission to journals published by the American Physical Society Downloadable here</li> <li>Presentation does NOT have to be given by all group members</li> </ul> </li> <li>Talk or email with Jason if you have questions about the appropriateness of your article</li> <li>Be sure to put down which article you are using here to avoid duplication</li> <li>Example presentation on Finite Monte Carlo article</li> <li>Other example articles (and no, you cannot use any of these articles for your report/presentation):<ul> <li>Frequency Difference Gating: A Multivariate Method for Identifying Subsets That Differ Between Samples (article)</li> <li>Probability binning comparison: a metric for quantitating multivariate distribution differences (article)</li> <li>FIREFLY MONTE CARLO: EXACT MCMC WITH SUBSETS OF DATA (article)</li> <li>This is just a small sample. Ideally, find something related to your area of research.</li> </ul> </li> <li>Include people names and article</li> <li>The 1-2 page summary as a .pdf file is due via email. Submission date is March 6 by 16:00 CET</li> <li>If you have any questions or concerns email Jason.</li> </ul>"},{"location":"#graded-problem-sets","title":"Graded problem sets (20%)","text":"<ul> <li>Problem set 1 (5% of total grade)<ul> <li>Due: Feb. 13, 2019 by 08:30 CET</li> </ul> </li> <li>Problem set 2 (15% of total grade)<ul> <li>Due: March 25, 2019 by 16:00 CET</li> <li>Partial Solutions for Problem Set 2</li> </ul> </li> </ul>"},{"location":"#project","title":"Project (30%)","text":"<ul> <li>Similar to the oral presentation, this project focuses on using a method or statistical treatment that is nominally related to your field of research that you or your group select. Unlike the oral presentation, the project includes not just understanding and explaining the method, but also using it on a some appropriate data set of your own choosing.</li> <li>Can be done alone or in groups of up to 3 people</li> <li>The only hand-in is a 4-6 page written report. You can submit the code as well if you would like.</li> <li>The project should be formatted and written as if it were a conference proceeding<ul> <li>Abstract, introduction, formatted figures w/ captions, citations, etc...</li> <li>I strongly encourage people to use LaTeX for the typesetting. For those who do not already have a style, I would recommend trying the formatting style for submission to journals published by the American Physical Society Downloadable here</li> <li>Here is an example of a fantastic write-up from 2017.</li> </ul> </li> <li>You may start working on this right now!!</li> <li>Due: April 2 by 22:00 CET</li> </ul>"},{"location":"#final-exam","title":"Final exam (40%)","text":"<ul> <li>Must work on your own!</li> <li>Take home exam<ul> <li>28 hour between start and submission</li> <li>Begins at 10:00 CET on Thursday April 4, 2019</li> <li>The exam must be submitted by 14:00 CET on Friday April 5, 2019</li> </ul> </li> <li>The exam will be similar to problem set 2<ul> <li>A handful of more intensive questions as opposed to numerous short questions</li> <li>While the exam will contain problems from any portion of the course material, the focus will be more on topics in the latter portion of the course</li> </ul> </li> <li>Here are two extra practice problems similar to what has been on the previous exams</li> <li>Here is a link to the 2016 exam</li> <li>Here is a link to the 2017 exam</li> </ul>"},{"location":"#extra-credit","title":"Extra Credit (+2% to final course grade average on a 1-100% scale)","text":"<ul> <li>2019 NCAA Men's Basketball Bracket submission due by tip-off of initial 1st round game on March 21</li> <li>This is NOT a requirement, nor is it an obligation for the course</li> <li>Information can be found here</li> <li>Due: March 21, 2019</li> </ul>"},{"location":"#course-syllabus","title":"Course Syllabus","text":"<p>The course is 100% likely to change once we begin, and future lectures listed below serve as an outline. Even so, we are very likely to cover the following topics which may require additional software support:</p> <ul> <li>Multivariate analysis (MVA) techniques including Boosted Decision Trees (BDTs)</li> <li>The MultiNest bayesian inference tool</li> <li>Basis splines</li> <li>Markov Chain Monte Carlo</li> <li>Likelihood minimization techniques</li> </ul>"},{"location":"StudentPresentations/","title":"Student Presentations for Advanced Methods in Applied Statistics 2019","text":"<ul> <li>The Efficiency of Geometric Samplers for Exoplanet Transit Timing Variation Models (presentation, write-up)</li> <li>Reverse time Monte Carlo: Development of models for atomic-scale dynamics (presentation, write-up)</li> <li>Applications of principal component analysis to pair distribution function (presentation (PDF, powerpoint), write-up)</li> <li>Statistical Paradises and Paradoxes in Big Data (presentation, write-up)</li> <li>ProbabilisticVisual Learning Object Detection (presentation, write-up)</li> <li>The persistent cosmic web and its filamentary structure- Theory, implementation (I) and illustrations (II) (presentation, write-up)</li> <li>Significance tests in climate science (presentation, write-up)</li> <li>Bayesian Inference of a Finite Population Mean Under Length-Biased Sampling (presentation, write-up)</li> <li>Wild Binary Segmentation (presentation, write-up)</li> <li>Simulated Annealing (presentation (PDF, powerpoint), write-up) movie is only in the powerpoint version</li> <li>Retrosynthesis - A reverse search tree (presentation, write-up)</li> <li>Random Forest (presentation (PDF, powerpoint), write-up)</li> </ul>"},{"location":"classes/","title":"Classes","text":"<p>Class notes will be posted here:</p>"},{"location":"classes/#0-pre-course","title":"0 - Pre-Course","text":"<ul> <li>Take a look before the class starts (optional)</li> <li>Get a preview with the course Teaching Assistant (Jean-Loup Tastet) of some software tools to install</li> <li>Lecture 0</li> </ul>"},{"location":"classes/#1-start-feb-5","title":"1 - Start (Feb. 5)","text":"<ul> <li>Course Information</li> <li>description in Kurser</li> <li>Chi-square</li> <li>Code chi-square</li> <li>Data for exercise 1 (FranksNumbers.txt)</li> <li>Review of 'basic' statistics</li> <li>Lecture 1<ul> <li>Jason's python code for exercise 1</li> <li>Jean-Loup's (2019 TA) python 3 code as a Jupyter notebook for exercise 1</li> </ul> </li> <li>Be knowledgeable about the Central Limit Theorem</li> <li>Start reading paper about how well Gaussian statistics compares to a wide selection of scientific measurements<ul> <li>\"Not Normal: the uncertainties of scientific measurements\" link at arXiv or DOI</li> <li>We will be discussion the paper in the next class, i.e. on Thursday</li> </ul> </li> </ul>"},{"location":"classes/#2-monte-carlo-simulation-least-squares-feb-7","title":"2 - Monte Carlo Simulation &amp; Least Squares (Feb. 7)","text":"<ul> <li>Lecture 2</li> <li>Monte Carlo (reminder that lecture starts at 09:00)</li> <li>Code for area of the circle</li> <li>Example code from Jean-Loup (2019 TA) in a Jupyter notebook</li> <li>From the \"Not Normal: the uncertainties of scientific measurements\" paper:<ul> <li>For the ambitious, create a 'toy monte carlo' of the sample and pair distributions for the nuclear physics data in Sec. 2.A. For simplicity assume that all the 'quantities' are gaussian distributed</li> <li>Write functions where you can produce multiple gaussian distributions to sample from and generate a sample of \"12380 measurements, 1437 quantities, 66677 pairs\".</li> <li>Produce the z-distribution (using eq. 4) plot for just your toy monte carlo and see if it matches a gaussian, exponential, student-t distribution, etc...</li> </ul> </li> <li>Least Squares lecture (starting at 13:00)</li> <li>Some useful links<ul> <li>Covariance Matrix (wiki)</li> <li>In-Depth (but still brief) least-squares write-up</li> </ul> </li> <li>Discussion of \"Not Normal: the uncertainties of scientific measurements\" (arXiv or DOI)</li> </ul>"},{"location":"classes/#3-introduction-to-likelihoods-and-numerical-minimizers-feb-12","title":"3 - Introduction to Likelihoods and Numerical Minimizers (Feb. 12)","text":"<ul> <li>Lecture 3</li> <li>Maximum likelihood method</li> <li>Gradient descent and minimizers</li> <li>Example code for exercise 1 and exercises 2 &amp; 3 from Jean-Loup (TA in 2018 &amp; 2019), Niccolo (TA in 2017), some from Jason (course lecturer)</li> <li>Remember that the first assignment is due on Wednesday</li> </ul>"},{"location":"classes/#4-intro-to-bayesian-statistics-splines-feb-14","title":"4 - Intro. to Bayesian Statistics &amp; Splines (Feb. 14)","text":"<ul> <li>Lecture 4 on Simple Bayesian statistics</li> <li>Using priors, posteriors, and likelihoods</li> <li>Example code for exercises from Jason</li> <li>Lecture 4.5</li> <li>Splines</li> <li>Data files for one of the exercises.<ul> <li>Dust Logger data</li> <li>Spline cubic data</li> <li>Spline oscillation data</li> </ul> </li> <li>Interesting article about use of splines and penalty terms<ul> <li>Penalized splines</li> </ul> </li> </ul>"},{"location":"classes/#5-parameter-estimation-and-confidence-intervals-feb-19","title":"5 - Parameter Estimation and Confidence Intervals (Feb. 19)","text":"<ul> <li>Lecture 5 Confidence intervals</li> <li>Numerical minimizers for best-fit values</li> <li>Data file for one of the exercises (extra data file)</li> <li>Reminder: oral presentation and 1-2 page article reports will be due/covered soon<ul> <li>Article about Supernova first detection time. Look at the caption for the Supplementary Fig. 8</li> </ul> </li> </ul>"},{"location":"classes/#6-markov-chains-feb-21","title":"6 - Markov Chain(s) (Feb. 21)","text":"<ul> <li>Look for an external package for Markov Chain Monte Carlo (MCMC), e.g. emcee, PyMC<ul> <li>Just like minimizers, syntax and options matter</li> <li>Be familiar with your chosen MCMC package</li> </ul> </li> <li>Lecture 6 Markov Chain Monte Carlo (MCMC)</li> <li>Some example python code for the exercises (caveat emptor)<ul> <li>Using PyMC, which wasn't the greatest package (at least in 2017 and 2018), but it got the job done</li> <li>Using emcee, the solution is graciously provided by Niccolo Maffezzoli (2017 TA)</li> </ul> </li> </ul>"},{"location":"classes/#7-hypothesis-testing-feb-26","title":"7 - Hypothesis Testing (Feb. 26)","text":"<ul> <li>Lecture 7</li> <li>Likelihood ratio</li> <li>Data files for one of the exercises. Just use the first column in each file. The second column is unimportant.<ul> <li>Data set 1</li> <li>Data set 2</li> </ul> </li> </ul>"},{"location":"classes/#8-data-driven-density-estimation-non-parametric-feb-28","title":"8 - Data Driven Density Estimation (non-parametric) (Feb. 28)","text":"<ul> <li>Kernel Density estimation</li> <li>Lecture 8</li> </ul>"},{"location":"classes/#9-confidence-intervals-failures-and-feldman-cousins-march-5","title":"9 - Confidence Intervals, Failures, and Feldman-Cousins (March 5)","text":"<ul> <li>Guest lecture by Dr. Morten Medici</li> <li>Under/over coverage in hypothesis tests</li> <li>Flip-flopping confidence intervals and corrections via ranking and use of Feldman-Cousins unified approach<ul> <li>Paper about unified approach by G. Feldman and R. Cousins</li> </ul> </li> </ul>"},{"location":"classes/#10-presentations-and-multivariate-analysis-techniques-march-7","title":"10 - Presentations and Multivariate Analysis techniques (March 7)","text":"<ul> <li>In the morning we will have the oral presentations from the articles chosen<ul> <li>Link to some of the 2019 presentations</li> <li>Links to some to some of the presentations (2016, 2017, 2018)</li> </ul> </li> <li>The Boosted Decision Tree lecture will be covered on March 14 in the afternoon due to the length of the excellent in-class student presentations and follow-up discussions.</li> </ul>"},{"location":"classes/#11-divergence-between-distributions-and-template-matching-march-12","title":"11 - Divergence Between Distributions and Template Matching (March 12)","text":"<ul> <li>Guest Lecture by Prof. Andrew \"Andy\" Jackson</li> <li>Lecture notes on Kullback-Leiber (part 1) and Template Matching (part 2) (PDF, powerpoint)</li> <li>Kullback-Leibler divergence as a way to compare the sameness (or tension) of two distributions, also known as a 'measure of surprise' or 'relative entropy'.<ul> <li>Kullback-Leibler exercises</li> <li>Relevant Publications:<ul> <li>Original publication by Kullback &amp; Leibler</li> <li>Application of Kullback-Leibler divergence for properties of the Cosmic Microwave Background</li> </ul> </li> </ul> </li> <li>Strict, or semi-strict, template matching compares data to predetermined 'templates'. Shortcomings of this approach will be covered.</li> </ul>"},{"location":"classes/#12-statistical-hypothesis-tests-auto-correlation-and-bdts-march-14","title":"12 - Statistical Hypothesis Tests, Auto-Correlation, and BDTs (March 14)","text":"<ul> <li>Guest lecture by Markus Ahlers<ul> <li>Lecture slides</li> <li>Files and some example code<ul> <li>Data files in .FITS format: eventmap1.fits and truemap1.fits</li> <li>Some example code (all in python): C1_produce.py C1_show.py KS_produce.py KS_show.py maxLH_produce.py maxLH_show.py powerspectrum.py twopoint.py Ylm.py</li> </ul> </li> <li>Be sure to have HEALPix software installed on your computer. There are options for C, C++, JAVA, Python, and I see some MATLAB too.</li> </ul> </li> <li>Boosted Decision Trees<ul> <li>Lecture 10</li> <li>Data<ul> <li>Exercise 1 (training signal, training background, testing signal, testing background)</li> <li>Exercise 2 (16 variable file)<ul> <li>The first column is the index, hence there are 17 'variables', but the index variable only for book keeping and has no impact on whether an event is signal or background.</li> <li>Every even row is the 'signal' and every odd row is the 'background'. Thus, there are two rows for each index in the first column: the first is the signal and the second is the background. [Format is odd, but I got it from a colleague].</li> </ul> </li> <li>Here is the solution data sets separated into two files (benign and malignant) for the last exercise of the lecture. Here is also the (python) code that I used to establish the efficiency for all the submissions from all the students</li> </ul> </li> </ul> </li> </ul>"},{"location":"classes/#13-nested-sampling-bayesian-inference-and-multinest-march-19","title":"13 - Nested Sampling, Bayesian Inference, and MultiNest (March 19)","text":"<ul> <li>Lecture 13</li> <li>External packages for conducting nested sampling, e.g. MultiNest, are necessary and some python options are:<ul> <li>pymultinest (https://johannesbuchner.github.io/PyMultiNest/)</li> <li>nestle (http://kbarbary.github.io/nestle/)</li> <li>SuperBayeS (http://www.ft.uam.es/personal/rruiz/superbayes/?page=main.html)</li> </ul> </li> <li>Very good articles that are easy to read<ul> <li>Excellent and readable paper by developer John Skilling on nested sampling (http://www.inference.phy.cam.ac.uk/bayesys/nest.pdf)<ul> <li>Read up until the section \"The Density of States\". There will be a discussion at the end of class.</li> </ul> </li> <li>MultiNest academic papers<ul> <li>http://arxiv.org/abs/0809.3437</li> <li>http://arxiv.org/abs/1306.2144</li> </ul> </li> </ul> </li> </ul>"},{"location":"classes/#14-work-on-project-march-21","title":"14 - Work on Project (March 21)","text":"<ul> <li>No lecture or new material.</li> </ul>"},{"location":"classes/#15-course-review-discussion-on-frequentist-and-bayesian-concepts-and-non-parametric-tests-lecture-snippet-march-26","title":"15 - Course Review, Discussion on Frequentist and Bayesian concepts, and Non-Parametric Tests Lecture snippet (March 26)","text":"<ul> <li>Review and recap of a few topics covered in the course</li> <li>Lecture 15 (EXTRA)<ul> <li>Kolmogorov-Smirnov, Anderson-Darling, and Mann-Whitney U tests</li> <li>Won't be be covered in class</li> <li>Topics include things that may be useful for research</li> </ul> </li> </ul> <p>Extra Projects of a more difficult nature, for those who want something more challenging.</p> <ul> <li>Parameter Goodness-of-fit (PG) in Global physics fits</li> </ul>"}]}