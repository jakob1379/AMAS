{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Advanced Methods in Applied Statistics 2017","text":""},{"location":"#the-lecturer","title":"The Lecturer","text":"<ul> <li> D. Jason Koskinen     Email: <code>koskinen (at) nbi.ku.dk</code></li> </ul>"},{"location":"#basic-information","title":"Basic Information","text":"<ul> <li>Block 3 - Timetable A of the 2017 academic calendar<ul> <li>Tues 08:00 - 12:00 and Thurs 08:00- 12:00 &amp; 13:00 - 17:00</li> <li>Actual<ul> <li>08:00 - 08:30 student study time for both Tues. and Thurs.</li> <li>08:30 - 09:00 Q&amp;A or discussion with Jason in Aud. M</li> <li>09:00 lecture on new material (not 09:05 or 09:15)</li> <li>On Thursday there will very often be new material starting at 13:00</li> <li>On Thursday it is very unlikely that any new material, lectures, or review will happen after 16:00.</li> </ul> </li> </ul> </li> <li>Auditorium M at the Blegdamsvej campus</li> <li>Odd-numbered classes are 4-hours while even-numbered consist of 2 blocks of 4-hours</li> <li>Classes will be composed of ~20-30% lecture and demonstrations followed by exercise</li> <li>While assignments, projects, and exercises can be done in the programming language of the students choice, the examples and demonstrations will be mainly in Python and/or scientific packages thereof, i.e. SciPy, PyROOT, etc.</li> <li>Required text or textbooks: None</li> <li>2016 Advanced Methods in Applied Statistics webpage</li> <li>It is recommended, but not required, to have at least reviewed the little sibling to this course, i.e. \"Applied Statistics - From data to results\" which can be found here</li> </ul>"},{"location":"#evaluation","title":"Evaluation","text":""},{"location":"#oral-presentation","title":"Oral presentation and 1-2 page summary (10%)","text":"<ul> <li>~10 minute summary presentation. Plan on ~7 slides if you are doing a PowerPoint-type presentation.</li> <li>Can work alone or in groups of up to 3.<ul> <li>A 1-2 page summary including any and all group members names</li> <li>Presentation does NOT have to be given by all group members</li> </ul> </li> <li>Talk or email with Jason if you have questions about the appropriateness of your article</li> <li>Be sure to put down which article you are using here to avoid duplication</li> <li>Example presentation on Finite Monte Carlo article</li> <li>Other example articles (and no, you cannot use any of these articles for your report/presentation):<ul> <li>Frequency Difference Gating: A Multivariate Method for Identifying Subsets That Differ Between Samples (article)</li> <li>Probability binning comparison: a metric for quantitating multivariate distribution differences (article)</li> <li>FIREFLY MONTE CARLO: EXACT MCMC WITH SUBSETS OF DATA (article)</li> <li>This is just a small sample. Find something related to your area of physics.</li> </ul> </li> <li>Include people names and article here by March 3</li> <li>The 1-2 page summary as a .pdf file is due via email. Submission date is March 8 by 16:00 CET</li> <li>Presentations will be selected at random and begin during class time on March 9. At the discretion of Jason and if needed, some presentations will be postponed for a later date.</li> <li>If you have any questions or concerns email Jason</li> </ul>"},{"location":"#graded-problem-sets","title":"Graded problem sets (20%)","text":"<ul> <li>Problem set 1 (5% of total grade)<ul> <li>Description</li> </ul> </li> <li>Problem set 2 (15% of total grade)<ul> <li>Description</li> <li>Data file filled with decay times for problem 2</li> <li>Due:Monday March 20 at 16:00 CET</li> </ul> </li> </ul>"},{"location":"#project","title":"Project (30%)","text":"<ul> <li>Similar to the oral presentation, this project focuses on using a method or statistical treatment related to your field of physics research that you or your group select. Unlike the oral presentation, the project includes not just understanding and explaining the method, but also using it on a some appropriate data set of your own choosing.</li> <li>Can be done alone or in groups of up to 3 people</li> <li>The only hand-in is a 4-6 page written report. You can submit the code as well if you would like.</li> <li>Due: Monday March 27 at 16:00 CET</li> </ul>"},{"location":"#final-exam","title":"Final exam (40%)","text":"<ul> <li>Must work on your own!</li> <li>Take home exam<ul> <li>28 hour between start and submission</li> <li>Begins at 13:00 CET on Thursday March 30, 2017</li> <li>The exam must be submitted by 17:00 CET on Friday March 31, 2017</li> </ul> </li> <li>The exam will be similar to problem set 2<ul> <li>A handful of more intensive questions as opposed to numerous short questions</li> <li>While the exam will contain problems from any portion of the course material, the focus will be more on topics in the latter portion of the course</li> </ul> </li> <li>Here are two extra practice problems similar to what will be on the 2017 exam</li> <li>Here is a link to the 2016 exam<ul> <li>Note that the 2017 exam will focus more on latter topics in the course than the 2016 version</li> <li>Also, the 2017 exam will be comparatively more difficult than the 2016 version</li> </ul> </li> <li>EXAM LINK IS HERE</li> </ul>"},{"location":"#extra-credit","title":"Extra Credit (+2% to final course grade average on a 1-100% scale)","text":"<ul> <li>2017 NCAA Men's Basketball Bracket submission due by tip-off of initial 1st round game on March 16</li> <li>This is NOT a requirement, nor is it an obligation for the course</li> <li>Extra Credit Outline</li> <li>Due: Thursday March 16 by 17:00 CET</li> </ul>"},{"location":"#course-syllabus","title":"Course Syllabus","text":"<p>The outline is a rough sketch of the course material, and is 100% likely to change throughout the course. Even so, we are very likely to cover the following topics which may require additional software support:</p> <ul> <li>Multivariate analysis (MVA) techniques including Boosted Decision Trees (BDTs)</li> <li>The MultiNest bayesian inference tool</li> <li>Basis splines</li> <li>Markov Chain Monte Carlo</li> <li>Likelihood minimization techniques</li> </ul>"},{"location":"classes/","title":"Classes","text":"<p>Class notes will be posted here:</p>"},{"location":"classes/#0-pre-course-attendance-is-not-required-feb-2-2017","title":"0 - Pre-Course, attendance is not required (Feb. 2, 2017)","text":"<ul> <li>Optional time to make sure your laptop is setup</li> <li>10:00-12:00 at Blegdamsvej (in Aud. B)</li> <li>Lecture 0</li> </ul>"},{"location":"classes/#1-start","title":"1 - Start","text":"<ul> <li>Course Information</li> <li>Chi-square</li> <li>Code chi-square</li> <li>Data for exercise 1 (FranksNumbers.txt)</li> <li>Review of 'basic' statistics</li> <li>Lecture 1<ul> <li>Jason's python code for exercise 1</li> </ul> </li> <li>Be knowledgeable about the Central Limit Theorem</li> <li>Start reading paper about how well Gaussian statistics compares to a wide selection of scientific measurements<ul> <li>\"Not Normal: the uncertainties of scientific measurements\" link at arXiv or DOI</li> <li>If there's time, there may be discussion on Thurs. about the paper</li> </ul> </li> <li>First problem set is assigned</li> </ul>"},{"location":"classes/#2-monte-carlo-simulation-least-squares-regression","title":"2 - Monte Carlo Simulation &amp; Least Squares regression","text":"<ul> <li>Lecture 2</li> <li>Monte Carlo (starting at 09:00)</li> <li>From the \"Not Normal: the uncertainties of scientific measurements\" paper:<ul> <li>For the ambitious, create a 'toy monte carlo' of the sample and pair distributions for the nuclear physics data in Sec. 2.A. For simplicity assume that all the 'quantities' are gaussian distributed</li> <li>Write functions where you can produce multiple gaussian distributions to sample from and generate a sample of \"12380 measurements, 1437 quantities, 66677 pairs\".</li> <li>Produce the z-distribution (using eq. 4) plot for just your toy monte carlo and see if it matches a gaussian, exponential, student-t distribution, etc...</li> </ul> </li> <li>Least Squares lecture (starting at 13:00)</li> <li>Discussion of the \"Not Normal: the uncertainties of scientific measurements\" paper</li> </ul>"},{"location":"classes/#3-introduction-to-likelihoods-and-numerical-minimizers","title":"3 - Introduction to Likelihoods and Numerical Minimizers","text":"<ul> <li>Lecture 3</li> <li>Maximum likelihood method</li> <li>Gradient descent and minimizers</li> <li>Example code from Niccolo (TA) and some from Jason (course lecturer)</li> <li>Remember that the first assignment is due on Wednesday</li> </ul>"},{"location":"classes/#4-finish-introduction-likelihoods-and-minimizers-then-intro-to-bayesian-statistics","title":"4 - Finish Introduction Likelihoods and Minimizers, then Intro. to Bayesian Statistics","text":"<ul> <li>Finish any material from previous class on likelihoods and minimizers</li> <li>Lecture 4 on Simple Bayesian statistics</li> <li>Using priors, posteriors, and likelihoods</li> <li>Example code for exercises from Jason</li> </ul>"},{"location":"classes/#5-background-subtraction-and-splots","title":"5 - Background Subtraction and sPlots","text":"<ul> <li>Lecture 5 (by Troels Peteresen)</li> <li>Data file for the exercises</li> <li>Scripts - sWeights.py and sWeights_solution.py</li> <li>For next class have an external package for Markov Chain Monte Carlo (MCMC), e.g. emcee, PyMC</li> </ul>"},{"location":"classes/#6-markov-chains","title":"6 - Markov Chain(s)","text":"<ul> <li>Be sure to have an external package for Markov Chain Monte Carlo (MCMC), e.g. emcee, PyMC<ul> <li>Just like minimizers, syntax and options matter</li> <li>Be somewhat familiar with your chosen MCMC package</li> </ul> </li> <li>Lecture 6 Markov Chain Monte Carlo (MCMC)</li> <li>Some example python code for the exercises (caveat emptor)<ul> <li>Using PyMC, which wasn't the greatest package (at least last year), but it got the job dones</li> <li>Using emcee, the solution is graciously provided by Niccolo Maffezzoli (TA)</li> </ul> </li> </ul>"},{"location":"classes/#7-parameter-estimation-and-confidence-intervals","title":"7 - Parameter Estimation and Confidence Intervals","text":"<ul> <li>Lecture 7 Confidence intervals</li> <li>Numerical minimizers for best-fit values</li> <li>Data file for one of the exercises</li> <li>Oral presentation and 1-2 page article reports will be due/covered March 8&amp;9<ul> <li>Article about Supernova first detection time. Look at the caption for the Supplementary Fig. 8</li> </ul> </li> </ul>"},{"location":"classes/#8-hypothesis-testing","title":"8 - Hypothesis Testing","text":"<ul> <li>Lecture 8</li> <li>Likelihood ratio</li> <li>Data files for one of the exercises. Just use the first column in each file. The second column is unimportant.<ul> <li>Data set 1</li> <li>Data set 2</li> </ul> </li> </ul>"},{"location":"classes/#9-interpolation-and-splines","title":"9 - Interpolation and Splines","text":"<ul> <li>Lecture 9</li> <li>Splines</li> <li>Data files for one of the exercises.<ul> <li>Dust Logger data</li> <li>Spline cubic data</li> <li>Spline oscillation data</li> </ul> </li> <li>Interesting article about use of splines and penalty terms<ul> <li>Penalized splines</li> </ul> </li> </ul>"},{"location":"classes/#10-presentations-and-multivariate-analysis-techniques","title":"10 - Presentations and Multivariate Analysis techniques","text":"<ul> <li>In the morning we will have the oral presentations from the articles chosen:<ul> <li>sFit: a method for background subtraction in maximum likelihood fit</li> <li>On a paradoxical property of the Kolmogorov-Smirnov two-sample test</li> <li>Too good to be true:When overwhelming evidence fails to convince (PPTX, PDF) (arXiv:1601.00900)</li> <li>Bayesian Interpolation (Paper)</li> <li>Chempy: A flexible chemical evolution model for abundance fitting (PDF, ODP)</li> </ul> </li> <li>Boosted Decision Trees</li> <li>Lecture 10</li> <li>Exercise 1 TMVA python script (will appear next week)</li> <li>Exercise 2 TMVA python script (will appear next week)</li> <li>Data<ul> <li>Exercise 1 (training signal, training background, testing signal, testing background)</li> <li>Exercise 2 (16 variable file)<ul> <li>The first column is the index, hence there are 17 'variables', but the index variable only for book keeping and has no impact on whether an event is signal or background.</li> <li>Every even row is the 'signal' and every odd row is the 'background'. Thus, there are two rows for each index in the first column: the first is the signal and the second is the background. [Format is odd, but I got it from a colleague].</li> </ul> </li> <li>Here is the solution data sets separated into two files (benign and malignant) for the last exercise of the lecture. Here is also the (python) code that I used to establish the efficiency for all the submissions from all the students</li> </ul> </li> </ul>"},{"location":"classes/#11-data-driven-density-estimation-non-parametric","title":"11 - Data Driven Density Estimation (non-parametric)","text":"<ul> <li>Kernel Density estimation</li> <li>Lecture 11</li> <li>Extra credit is now available (see here)</li> <li>Problem set #2 is now assigned (see here)</li> </ul>"},{"location":"classes/#12-confidence-intervals-failures-and-feldman-cousins","title":"12 - Confidence Intervals, Failures, and Feldman-Cousins","text":"<ul> <li>Guest lecture by Dr. Morten Medici</li> <li>Under/over coverage in hypothesis tests</li> <li>Flip-flopping confidence intervals and corrections via ranking and use of Feldman-Cousins unified approach<ul> <li>Paper about unified approach by G. Feldman and R. Cousins</li> </ul> </li> <li>Lecture 12</li> <li>Yes, this topic may appear on the exam :-)</li> <li>I have posted the solution data sets to the webpage for the BDT classification exercise (see links for Class 10 above)</li> <li>The due date for the project is now March 27, 2017</li> </ul>"},{"location":"classes/#13-nested-sampling-bayesian-inference-and-multinest","title":"13 - Nested Sampling, Bayesian Inference, and MultiNest","text":"<ul> <li>Lecture 13</li> <li>External packages for conducting nested sampling, e.g. MultiNest, are necessary and some python options are:<ul> <li>pymultinest (https://johannesbuchner.github.io/PyMultiNest/)</li> <li>nestle (http://kbarbary.github.io/nestle/)</li> <li>SuperBayeS (http://www.ft.uam.es/personal/rruiz/superbayes/?page=main.html)</li> </ul> </li> <li>Super awesome articles that are surprisingly easy to read<ul> <li>Excellent and readable paper by developer John Skilling on nested sampling (http://www.inference.phy.cam.ac.uk/bayesys/nest.pdf)</li> <li>MultiNest papers<ul> <li>http://arxiv.org/abs/0809.3437</li> <li>http://arxiv.org/abs/1306.2144</li> </ul> </li> </ul> </li> </ul>"},{"location":"classes/#14-signal-and-data-processing-wavelets","title":"14 - Signal and Data Processing (Wavelets)","text":"<ul> <li>Guest lecture by Dr. James Monk</li> <li>To prepare for the class make sure that a wavelet package is available<ul> <li>For example in Python - \"pip install PyWavelets\"</li> <li>Matlab - http://se.mathworks.com/products/wavelet/</li> </ul> </li> <li>Lecture 14</li> <li>Some coding scripts<ul> <li>Gaussian (part1, part2, part3)</li> <li>LIGO<ul> <li>H-H1_LOSC_4_V1-1126259446-32.txt</li> <li>L-L1_LOSC_4_V1-1126259446-32.txt</li> </ul> </li> </ul> </li> </ul>"},{"location":"classes/#15-non-parametric-tests-lecture-snippet-and-course-review","title":"15 - Non-Parametric Tests Lecture snippet and Course Review","text":"<ul> <li>Kolmogorov-Smirnov, Anderson-Darling, and Mann-Whitney U tests</li> <li>Lecture 15</li> <li>Review and recap of a few topics covered in the course</li> </ul> <p>Extra Projects of a more difficult nature, for those who want something more challenging.</p> <ul> <li>Parameter Goodness-of-fit (PG) in Global physics fits</li> </ul>"}]}