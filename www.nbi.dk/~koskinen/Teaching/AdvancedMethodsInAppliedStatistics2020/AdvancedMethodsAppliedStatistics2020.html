<!DOCTYPE html>
<html>

<head>
  <meta content="text/html; charset=windows-1252" http-equiv="content-type">
  <title>AdvancedMethodsAppliedStatistics</title>
  <meta http-equiv="refresh" content="300">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
</head>

<body>
  <h1>Advanced Methods in Applied Statistics 2020</h1>
  <br>
  <img
    src="https://ÿÿÿ/DJK.jpeg"
    alt=""><br>
  Lecturer: D. Jason Koskinen<br>
  Email: koskinen (at) nbi.ku.dk<br>
  <h2>Basic Information</h2>
  <ul>
    <li>Block 3 - Timetable A of the 2020 <a href="http://www.science.ku.dk/english/student-life/studying-at-the-faculty/academic-calendar/">academic
        calendar</a></li>
    <ul>
      <li>Tues 08:00 - 12:00 and Thurs 08:00- 12:00 &amp; 13:00 - 17:00</li>
      <li>Actual</li>
      <ul>
        <li>08:30 - 09:00 Q&amp;A or discussion with Jason in the classroom</li>
        <li><b>09:00</b> lecture on new material (not 09:05 or 09:15)</li>
        <li>On Thursday there will very often be new material starting at 13:00</li>
        <li>On Thursday it is very unlikely that any new material, lectures, or review will happen after 16:00. </li>
      </ul>
    </ul>
    <li>There are multiple locations depending on the day (<a title="timetable"
        href="../../../../skema.ku.dk/tt/tt.asp%3FSDB=KU1920&language=DK&folder=Reporting&style=textspreadsheet&type=module&idtype=id&id=92118&weeks=27-52&days=1-5&periods=5-52&width=0&height=0&template=SWSCUST2%20module%20textspreadsheet.html">timetable</a>):
    </li>
    <ul>
      <li><del>Tuesday is at C103, Universitetsparken 5, HCÃ˜</del></li>
      <li><b>Tuesday</b> is at NBB.01.01.G076, Jagtvej 155</li>
      <li>Thursday morning is at NBB 01.3.I.164, Jagtvej 155</li>
      <li>Thursday afternoon is at AB Teori 2, NÃ¸rre Alle 55</li>
    </ul>
    <li>Odd-numbered classes are 4-hours while even-numbered consist of 2 blocks of 4-hours</li>
    <li>Classes will be composed of ~20-30% lecture and demonstrations followed by exercise</li>
    <li>While assignments, projects, and exercises can be done in the programming language of the students choice, the examples and demonstrations will be mainly in Python and/or scientific packages thereof, i.e. SciPy, PyROOT, etc.</li>
    <li>Required text or textbooks: None</li>
    <li>2016 Advanced Methods in Applied Statistics <a href="../AdvancedMethodsInAppliedStatistics2016/AdvancedMethodsAppliedStatistics2016.html">webpage</a></li>
    <li>2017 Advanced Methods in Applied Statistics <a href="../AdvancedMethodsInAppliedStatistics2017/AdvancedMethodsAppliedStatistics2017.html">webpage</a></li>
    <li>2018 Advanced Methods in Applied Statistics <a href="../AdvancedMethodsInAppliedStatistics2018/AdvancedMethodsAppliedStatistics2018.html">webpage</a></li>
    <li>2019 Advanced Methods in Applied Statistics <a href="../AdvancedMethodsInAppliedStatistics2019/AdvancedMethodsAppliedStatistics2019.html">webpage</a></li>
    <li><span style="color: #0000ee;"></span>It is recommended, but not required, to have at least reviewed the little sibling to this course, i.e. "Applied Statistics - From data to results" which can be found <a
        href="http://www.nbi.dk/%7Epetersen/Teaching/AppliedStatistics2019.html">here</a></li>
  </ul>

  <hr>

  <h1>Evaluation</h1>
  The presentation, the problems sets, and the project will all be submitted and assigned from Absalon. So check Absalon for instructions and due dates. The final exam is handled by the eksamen webpage.
  <ul>
    <li> <span style="text-decoration: underline; font-weight: bold;">Oral
        presentation and 1-2 page summary</span> (10%)</li>
    <li><span style="text-decoration: underline; font-weight: bold;">Graded
        problem sets</span> (20%)</li>
    <li><span style="text-decoration: underline; font-weight: bold;">Project</span>
      (30%)</li>
    <ul>
      <li>You may start working on <b>this right now!!</b></li>
    </ul>
    <li><span style="text-decoration: underline; font-weight: bold;">Final exam</span> (40%)</li>
    <ul>
      <li>28 hour take home exam</li>
      <li>The exam will be similar to problem set 2</li>
      <ul>
        <li>A handful of more intensive questions as opposed to numerous
          short questions</li>
        <li>While the exam will contain problems from any portion of the
          course material, the focus will be more on topics in the latter
          portion of the course</li>
      </ul>
      <li><a href="ExtraProblems.pdf">Here</a>
        are two extra practice problems and the exams for <a href="Exam_2016.pdf">2016 </a> and <a
          href="Exam_2017.pdf">2017</a></li>
    </ul>
    <li><span style="text-decoration: underline; font-weight: bold;"><del>Extra Credit</del></span><del> (+2% to final course grade average on a 1-100% scale)</del></li>
    <ul>
    </ul>
  </ul>

  <hr>

  <h2>Course Syllabus</h2>
  <p>The course is 100% likely to change once we begin, and future lectures
    listed below serve as an outline. Even so, we <span style="font-weight: bold;">are
      very likely to</span> cover the following topics which may require
    additional software support:</p>
  <ul>
    <li>Multivariate analysis (MVA) techniques including Boosted Decision
      Trees (BDTs)</li>
    <li>The MultiNest bayesian inference tool</li>
    <li>Basis splines</li>
    <li>Markov Chain Monte Carlo</li>
    <li>Likelihood minimization techniques</li>
  </ul>
  Class notes will be posted here on this webpage as they become available.<br>
  <br><b>Class 0 - Pre-Course (Jan. 30)</b>
  <ul>
    <li>The location is Fb-6 at Blegdamsvej from 13:00-16:00</li>
    <li>Take a look before the class starts (optional)</li>
    <li>Get a preview with the course Teaching Assistant (James Creswell) of
      some software tools to install</li>
  </ul>
  <hr> <b>Class 1 - Start (Feb. 4)</b><br>
  <ul>
    <li><a href="https://ÿÿÿ/CourseInformation.pdf">Course Information</a> </li>
    <li>Chi-square </li>
    <li>Code chi-square</li>
    <li>Data for exercise 1 (<a href="../AdvancedMethodsInAppliedStatistics2018/data/FranksNumbers.txt">FranksNumbers.txt</a>)</li>
    <li>Review of 'basic' statistics</li>
    <li><a href="https://ÿÿÿ/Lecture1_Basics_ChiSquare.pdf">Lecture 1</a></li>
    <ul>
      <li><span style="text-decoration: underline;"></span>Jason's <a href="https://ÿÿÿ/Exercises/Lecture1_Variance.py">python
          code</a> for exercise 1</li>
      <li>Jean-Loup's (2019 TA) <a href="https://ÿÿÿ/Exercises/Lecture1_Variance_Py3.ipynb">python
          3 code as a Jupyter notebook</a> for exercise 1</li>
    </ul>
    <li>Be knowledgeable about the Central Limit Theorem</li>
    <li>Start reading paper about how well Gaussian statistics compares to a
      wide selection of scientific measurements</li>
    <ul>
      <li>"Not Normal: the uncertainties of scientific measurements" link at <a href="https://arxiv.org/abs/1612.00778">arXiv</a>
        or <a href="http://rsos.royalsocietypublishing.org/content/4/1/160600">DOI</a></li>
      <li>We will be discussion the paper in the next class, i.e. on Thursday</li>
    </ul>
  </ul>

  <hr>

  <b>Class 2 - Monte Carlo Simulation &amp; Least Squares (Feb. 6)</b><br>
  <ul>
  </ul>
  <ul>
    <li><a href="https://ÿÿÿ/Lecture2_MC_LeastSquares.pdf">Lecture 2</a></li>
    <li>Monte Carlo (reminder that lecture starts at 09:00)</li>
    <li>Code for <a href="https://ÿÿÿ/Exercises/Lecture2_CircleArea.py">area of the
        circle</a></li>
    <li><a href="https://ÿÿÿ/Exercises/Lecture2_CircleArea_Py3.ipynb">Example cod</a>e
      from Jean-Loup in a Jupyter notebook</li>
    <li>From the "Not Normal: the uncertainties of scientific measurements" <a href="https://arxiv.org/abs/1612.00778">paper</a>:</li>
    <ul>
      <li>For the ambitious, create a 'toy monte carlo' of the sample and pair
        distributions for the nuclear physics data in Sec. 2.A. For simplicity
        assume that all the 'quantities' are gaussian distributed</li>
      <li>Write functions where you can produce multiple gaussian
        distributions to sample from and generate a sample of "12380
        measurements, 1437 quantities, 66677 pairs".</li>
      <li>Produce the z-distribution (using eq. 4) plot for just your toy
        monte carlo and see if it matches a gaussian, exponential, student-t
        distribution, etc...</li>
    </ul>
  </ul>
  <ul>
    <li>Least Squares lecture (starting at 13:00)</li>
    <li>Some useful links</li>
    <ul>
      <li><a href="https://en.wikipedia.org/wiki/Covariance_matrix">Covariance
          Matrix (wiki)</a></li>
      <li><a href="http://stat.ethz.ch/%7Egeer/bsa199_o.pdf">In-Depth (but
          still brief) least-squares write-up</a></li>
    </ul>
  </ul>
  <ul>
    <li>Discussion of "Not Normal: the uncertainties of scientific
      measurements" (<a href="https://arxiv.org/abs/1612.00778">arXiv</a> or <a href="http://rsos.royalsocietypublishing.org/content/4/1/160600">DOI</a>)</li>
  </ul>

  <hr>

  <b>Class 3 - Introduction to Likelihoods and Numerical Minimizers (Feb. 11)<br>
  </b>
  <ul>
    <li><a href="https://ÿÿÿ/Lecture3_General_Likelihood.pdf">Lecture 3</a></li>
    <li>Maximum likelihood method</li>
    <li>Gradient descent and minimizers</li>
    <li>Example code for <a href="https://ÿÿÿ/Exercises/Lecture3_Exercise1.ipynb">exercise
        1</a> and <a href="https://ÿÿÿ/Exercises/Lecture3_Exercises2-3.ipynb">exercises 2
        &amp; 3</a> from Jean-Loup (TA in 2018 &amp; 2019),&nbsp; <a href="https://ÿÿÿ/Exercises/Lecture3_likelihood_niccolo.py">Niccolo</a>
      (TA in 2017), some from <a href="https://ÿÿÿ/Exercises/Lecture3_MLE_Cowan_clean.py">Jason</a>
      (course lecturer)</li>
  </ul>
  <ul>
    <li>Remember that the first assignment is due on <b> Wednesday</b></li>
  </ul>
  <br>

  <hr>

  <b>Class 4 - Intro. to Bayesian Statistics &amp; Splines (Feb. 13)<br></b>
  <ul>
    <li><a href="https://ÿÿÿ/Lecture4_Bayes.pdf">Lecture 4</a> on Simple Bayesian statistics</li>
    <li>Using priors, posteriors, and likelihoods</li>
    <li>Example <a href="https://ÿÿÿ/Exercises/Lecture4_Bayes_1.py">code</a> for
      exercises from Jason</li>
  </ul>
  <ul>
    <li><a href="https://ÿÿÿ/Lecture4.5_Splines.pdf">Lecture 4.5<span style="color: black;"></span></a>
    </li>
    <li>Splines</li>
    <li>Data files for one of the exercises. </li>
    <ul>
      <li><a href="https://ÿÿÿ/data/DustLog_forClass.dat">Dust Logger data<br>
        </a></li>
      <li><a href="https://ÿÿÿ/data/SplineCubic.txt">Spline cubic data</a></li>
      <li><span style=" color: #0000ee;"><a href="https://ÿÿÿ/data/SplineOsc1.txt">Spline oscillation data</a></span></li>
    </ul>
    <li>Interesting article about use of splines and penalty terms</li>
    <ul>
      <li><a href="https://arxiv.org/pdf/1301.2184v1.pdf">Penalized splines</a></li>
    </ul>
  </ul>

  <hr>

  <p><b>Class 5 - </b><b>Parameter Estimation and Confidence Intervals (Feb. 18)</b> </p>
  <ul>
    <li><a href="https://ÿÿÿ/Lecture5_ConfidenceIntervals.pdf">Lecture 5</a> Confidence intervals</li>
    <li>Numerical minimizers for best-fit values</li>
    <li><a href="https://ÿÿÿ/data/MLE_Variance_data.txt">Data file</a> for one of the exercises (<a href="https://ÿÿÿ/data/MLE_Variance_data_2.txt">extra data file</a>)</li>
  </ul>
  <ul>
    <li>Reminder: oral presentation and 1-2 page article reports will be due/covered soon</li>
    <ul>
      <li><a href="https://arxiv.org/abs/1701.02596">Article about Supernova</a> first detection time. Look at the caption for the Supplementary Fig. 8</li>
    </ul>
  </ul>

  <hr>

  <b>Class 6 - Markov Chain(s) (Feb. 20)<br></b>
  <ul>
    <li><a href="https://ÿÿÿ/Lecture6_MCMC_Bayes.pdf">Lecture 6</a> Markov Chain Monte Carlo (MCMC)</li>
    <li>Look for an external package for Markov Chain Monte Carlo (MCMC), e.g. emcee, PyMC</li>
    <ul>
      <li>Just like minimizers, syntax and options matter</li>
      <li>Be familiar with your chosen MCMC package</li>
    </ul>
    <li>Some example python code for the exercises (caveat emptor)</li>
    <ul>
      <li><a href="https://ÿÿÿ/Exercises/Lecture6_MCMC_Example1_Niccolo.py">Using emcee</a>, the solution is graciously provided by Niccolo Maffezzoli (2017 TA)</li>
    </ul>
  </ul>

  <hr>

  <b>Class 7 - </b><b>Hypothesis Testing (Feb. 25)</b>
  <ul>
      <li><a href="https://ÿÿÿ/Lecture7_HypothesisTests.pdf">Lecture 7</a> </li>
      <li>Likelihood ratio</li>
    <li>Data files for one of the exercises. Just use the first column in each file. The second column is unimportant.</li>
    <ul>
      <li><a href="https://ÿÿÿ/data/LLH_Ratio_2_data.txt">Data set 1</a></li>
      <li><a href="https://ÿÿÿ/data/LLH_Ratio_2a_data.txt">Data set 2</a></li>
    </ul>
  </ul>

  <hr>

  <p><b>Class 8 - Data Driven Density Estimation (non-parametric) (Feb. 27)</b> </p>
  <ul>
    <li>Kernel Density estimation</li>
    <li><a href="https://ÿÿÿ/Lecture8_KDE.pdf">Lecture 8</a></li>
  </ul>

  <hr>

  <p><b>Class 9 - Statistical Hypothesis Tests and Auto-Correlation (March 3)</b></p>
  <ul>
    <li>Guest lecture by <a href="https://nbia.nbi.ku.dk/members/ahlers/">Markus Ahlers</a></li>
    <li><a href="https://ÿÿÿ/Lecture_Ahlers2020.pdf">Lecture slides</a></li>
    <li>Files and some example code</li>
    <ul>
      <li>Data files in .FITS format: <a href="https://ÿÿÿ/data/eventmap1.fits">eventmap1.fits</a>
        and <a href="https://ÿÿÿ/data/truemap1.fits">truemap1.fits</a></li>
      <li>Some example code (all in python): <a href="https://ÿÿÿ/Exercises/C1_produce.py">C1_produce.py</a>
        <a href="https://ÿÿÿ/Exercises/C1_show.py">C1_show.py</a> <a href="https://ÿÿÿ/Exercises/KS_produce.py">KS_produce.py</a>
        <a href="https://ÿÿÿ/Exercises/KS_show.py">KS_show.py</a> <a href="https://ÿÿÿ/Exercises/maxLH_produce.py">maxLH_produce.py</a>
        <a href="https://ÿÿÿ/Exercises/maxLH_show.py">maxLH_show.py</a> <a href="https://ÿÿÿ/Exercises/powerspectrum.py">powerspectrum.py</a>
        <a href="https://ÿÿÿ/Exercises/twopoint.py">twopoint.py</a> <a href="https://ÿÿÿ/Exercises/Ylm.py">Ylm.py</a></li>
    </ul>
    <li><b>Be sure </b>to have <a href="https://healpix.jpl.nasa.gov">HEALPix software</a> installed on your computer. There are options for C, C++, JAVA, Python, and I see some MATLAB too.</li>
  </ul>
  <hr>

  <p><b>Class 10 - Presentations and Multivariate Analysis techniques (March 5)</b> </p>
  <ul>
    <li>In the morning we will have the oral presentations from the articles chosen</li>
    <ul>
      <li>Links to some to some of the presentations (<a href="https://ÿÿÿ/AdvancedMethodsInAppliedStatistics2016/Presentations_2016.html">2016</a>,
        <a href="https://ÿÿÿ/AdvancedMethodsInAppliedStatistics2017/StudentPresentations2017.html">2017</a>,
        <a href="https://ÿÿÿ/AdvancedMethodsInAppliedStatistics2018/StudentPresentations2018.html">2018</a>,
        <a href="https://ÿÿÿ/AdvancedMethodsInAppliedStatistics2019/StudentPresentations2019.html">2019</a>)</li>
    </ul>
  </ul>
  <p>The Boosted Decision Tree lecture</p>
  <ul>
    <li>Boosted Decision Trees</li>
    <li><a href="https://ÿÿÿ/Lecture10_MVA.pdf">Lecture 10</a></li>
    <li>Data</li>
    <ul>
      <li>Exercise 1 (<a href="https://ÿÿÿ/data/BDT_signal_train.txt">training signal</a>,
        <a target="_top" href="https://ÿÿÿ/data/BDT_background_train.txt">training
          background</a>, <a href="https://ÿÿÿ/data/BDT_signal_test.txt">testing signal</a>,
        <a href="https://ÿÿÿ/data/BDT_background_test.txt">testing background</a>)</li>
    </ul>
    <ul>
      <li>Exercise 2 (16 variable <a href="https://ÿÿÿ/data/BDT_16var.txt">file</a>)</li>
      <ul>
        <li>The first column is the index, hence there are 17 'variables', but the index variable only for book keeping and has no impact on whether an event is signal or background.
        </li>
        <li>Every even row is the 'signal' and every odd row is the 'background'. Thus, there are two rows for each index in the first column: the first is the signal and the second is the background. [Format is odd, but I got it from a colleague].
        </li>
      </ul>
      <li>Here is the solution data sets separated into two files (<a href="https://ÿÿÿ/data/benign_true.txt">benign</a>
        and <a href="https://ÿÿÿ/data/malignant_true.txt">malignant</a>) for the last exercise of the lecture. Here is also the&nbsp;<a href="https://ÿÿÿ/AdvancedMethodsInAppliedStatistics2017/Exam2_Problem_BDT_CheckSolutions_2016.py">(python) code</a> that I used to
        establish the efficiency for all the submissions from all the students</li>
    </ul>
  </ul>

  <hr>

  <p><b>Class 11 - Neural likelihood-free inference (March 10)</b></p>
  <ul>
    <li>Guest Lecture by <a href="http://christophweniger.com">Christoph Weniger</a></li>
    <li><a href="https://cweniger.github.io/teaching-2003-NBI-ML-lectures/lecture1.html">lecture notes</a></li>
    <li>Introduction to simple deep feed-forward networks</li>
    <li>Parameter regression</li>
    <li>Regression of likelihood-to-evidence ratios</li>
  </ul>

  <hr>

  <b>Class 12 - Continuation of likelihood-free inference (March 12)</b>
  <ul>
    <li>Guest lecture by <a href="http://christophweniger.com">Christoph Weniger</a></li>
    <li><a href="https://cweniger.github.io/teaching-2003-NBI-ML-lectures/lecture1.html">lecture notes</a></li>
    <li>Introduction to simple deep feed-forward networks</li>
    <li>Parameter regression</li>
    <li>Regression of likelihood-to-evidence ratios</li>
  </ul>

  <hr>

  <p><b>Class 13 - Nested Sampling, Bayesian Inference, and MultiNest (March 17)</b> </p>
  <ul>
    <li><a href="https://ÿÿÿ/Lecture13_MultiNest.pdf">Lecture 13</a></li>
    <li>External packages for conducting nested sampling, e.g. MultiNest, are necessary and some python options are:</li>
    <ul>
      <li>pymultinest (<a href="../../../../johannesbuchner.github.io/PyMultiNest/index.html">https://johannesbuchner.github.io/PyMultiNest/</a>)</li>
      <li>nestle (<a href="http://kbarbary.github.io/nestle/">http://kbarbary.github.io/nestle/</a>)</li>
      <li>SuperBayeS (<a href="http://www.ft.uam.es/personal/rruiz/superbayes/?page=main.html">http://www.ft.uam.es/personal/rruiz/superbayes/?page=main.html</a>)</li>
    </ul>
    <li>Very good articles that are easy to read</li>
    <ul>
      <li>Excellent and readable paper by developer John Skilling on nested sampling (<a href="http://www.inference.phy.cam.ac.uk/bayesys/nest.pdf">http://www.inference.phy.cam.ac.uk/bayesys/nest.pdf</a>)</li>
      <ul>
        <li><b>Read up until</b> the section "The Density of States"</li>
      </ul>
      <li>MultiNest academic papers</li>
      <ul>
        <li><a href="http://arxiv.org/abs/0809.3437">http://arxiv.org/abs/0809.3437</a></li>
        <li><a href="http://arxiv.org/abs/1306.2144">http://arxiv.org/abs/1306.2144</a></li>
      </ul>
    </ul>
  </ul>

  <hr>

  <p><b>Class 14 - Work on Project (no lecture or new material - March 19)<br>
    </b></p>

  <hr>

  <p><b>Class 15 - Course Review</b>, and Non-Parametric Tests Lecture snippet (March 24)<br> </b></p>
  <ul>
    <!---
    <li><a href="Lecture_Review.pdf">Review and recap</a> of a few topics
      covered in the course</li>
      --->
  </ul>
  <br>
  <ul>
    <li><a href="https://ÿÿÿ/Lecture15_Nonparameteric.pdf">Lecture 15</a> (EXTRA)</li>
    <ul>
      <li>Kolmogorov-Smirnov, Anderson-Darling, and Mann-Whitney U tests</li>
      <li><i>Won't be be covered in class</i></li>
      <li>Topics include things that may be useful for research</li>
    </ul>
  </ul>
  <br>
  <p>Extra Projects of a more difficult nature, for those who want something
    more challenging. </p>
  <ul>
    <li><a href="../AdvancedMethodsInAppliedStatistics2016/ProblemFromMIT.pdf">Parameter
        Goodness-of-fit</a> (PG) in Global physics fits</li>
  </ul>
  <p><br>
  </p>
  <p><br>
  </p>
  <ul>
  </ul>
  <dl>
  </dl>
</body>

</html>
